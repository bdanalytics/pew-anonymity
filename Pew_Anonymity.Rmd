---
# Get YAML keywords from myYAML_ref.Rmd
title: "Pew-Anonymity: <Predicted attribute> <regression/classification>"
author: "bdanalytics"
pandoc_args: ["+RTS", "-K64M", "-RTS"]

# Choose one:
output:
    html_document:
        keep_md: yes

# output:
#   pdf_document:
#     fig_width: 8
#     highlight: zenburn
#     #keep_md: yes
#     keep_tex: yes
#     number_sections: yes
#     toc: yes
---

**  **    
**Date: `r format(Sys.time(), "(%a) %b %d, %Y")`**    

# Introduction:  

Data: 
Source: 
    Training:   https://courses.edx.org/c4x/MITx/15.071x_2/asset/AnonymityPoll.csv  
    New:        <prdct_url>  
Time period: 

```{r set_global_options_wd, echo=FALSE}
setwd("~/Documents/Work/Courses/MIT/Analytics_Edge_15_071x/Assignments/HW1_Pew_Anonymity")
```

# Synopsis:

Based on analysis utilizing <> techniques, <conclusion heading>:  

### ![](<filename>.png)

## Potential next steps include:

# Analysis: 
```{r set_global_options}
rm(list=ls())
set.seed(12345)
options(stringsAsFactors=FALSE)
source("~/Dropbox/datascience/R/mydsutils.R")
source("~/Dropbox/datascience/R/myplot.R")
source("~/Dropbox/datascience/R/mypetrinet.R")
# Gather all package requirements here
#suppressPackageStartupMessages(require())

#require(sos); findFn("pinv", maxPages=2, sortby="MaxScore")

# Analysis specific global variables
glb_separate_predict_dataset <- FALSE

script_df <- data.frame(chunk_label="import_data", chunk_step_major=1, chunk_step_minor=0)
print(script_df)
```

## Step ``r script_df[nrow(script_df), "chunk_step_major"]``: import data
```{r import_data, cache=TRUE}
entity_df <- myimport_data(
    url="https://courses.edx.org/c4x/MITx/15.071x_2/asset/AnonymityPoll.csv", 
    comment="entity_df", print_diagn=TRUE)
if (glb_separate_predict_dataset) {
    predct_df <- myimport_data(
        url="<prdct_url>", 
        comment="predct_df", print_diagn=TRUE)
} else {
    predct_df <- entity_df[sample(1:nrow(entity_df), nrow(entity_df) / 1000),]
    comment(predct_df) <- "predct_df"
    myprint_df(predct_df)
    str(predct_df)
}         

script_df <- rbind(script_df, 
                   data.frame(chunk_label="inspect_data", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, 
                              chunk_step_minor=1))
print(script_df)
```

### Step ``r script_df[nrow(script_df), "chunk_step_major"]``.``r script_df[nrow(script_df), "chunk_step_minor"]``: inspect data
```{r inspect_data_1, cache=TRUE}
#print(str(entity_df))
#View(entity_df)

# List info gathered for various columns
# <col_name>:   <description>; <notes>

# Create new features that help diagnostics
#   Convert factors to dummy variables
#   Potential Enhancements:
#       One code chunk to cycle thru entity_df & predct_df ?
#           Use with / within ?
#           for (df in c(entity_df, predct_df)) cycles thru column names
#           for (df in list(entity_df, predct_df)) does not change the actual dataframes
#
#       Build splines   require(splines); bsBasis <- bs(training$age, df=3)

# entity_df <- mutate(entity_df,
#     <col_name>.NA=is.na(<col_name>) 
#     <col_name>_fctr=as.factor(<col_name>),
#     
#     Date.my=as.Date(strptime(Date, "%m/%d/%y %H:%M")),
#     Year=year(Date.my),
#     Month=months(Date.my),
#     Weekday=weekdays(Date.my)
#     
#                     )
# 
# predct_df <- mutate(predct_df, 
#                     )

print(summary(entity_df))
print(sapply(names(entity_df), function(col) sum(is.na(entity_df[, col]))))
print(summary(predct_df))
print(sapply(names(predct_df), function(col) sum(is.na(predct_df[, col]))))

#pairs(subset(entity_df, select=-c(col_symbol)))

#   Histogram of predictor in entity_df & predct_df
# Check for predct_df & entity_df features range mismatches

# Other diagnostics:
# print(subset(entity_df, <col1_name> == max(entity_df$<col1_name>, na.rm=TRUE) & 
#                         <col2_name> <= mean(entity_df$<col1_name>, na.rm=TRUE)))

print(Smartphone_freq_entity_df <- mycreate_tbl_df(entity_df, "Smartphone"))
print(State_freq_entity_df <- mycreate_tbl_df(subset(entity_df, Region == "South"), 
                                              "State"))
# print(which.min(table(entity_df$<col_name>)))
# print(which.max(table(entity_df$<col_name>)))
# print(which.max(table(entity_df$<col1_name>, entity_df$<col2_name>)[, 2]))
print(table(entity_df$Sex, entity_df$Region))
print(table(entity_df$State, entity_df$Region))
# print(table(is.na(entity_df$<col1_name>), entity_df$<col2_name>))
# print(xtabs(~ <col1_name>, entity_df))
# print(xtabs(~ <col1_name> + <col2_name>, entity_df))
# print(<col1_name>_<col2_name>_xtab_entity_df <- 
#   mycreate_xtab(entity_df, c("<col1_name>", "<col2_name>")))
# <col1_name>_<col2_name>_xtab_entity_df[is.na(<col1_name>_<col2_name>_xtab_entity_df)] <- 0
# print(<col1_name>_<col2_name>_xtab_entity_df <- 
#   mutate(<col1_name>_<col2_name>_xtab_entity_df, 
#             <col3_name>=(<col1_name> * 1.0) / (<col1_name> + <col2_name>))) 

# print(<col2_name>_min_entity_arr <- 
#    sort(tapply(entity_df$<col1_name>, entity_df$<col2_name>, min, na.rm=TRUE)))
# print(<col1_name>_na_by_<col2_name>_arr <- 
#    sort(tapply(entity_df$<col1_name>.NA, entity_df$<col2_name>, mean, na.rm=TRUE)))

print(sum((entity_df$Internet.Use == 0) & (entity_df$Smartphone == 0), na.rm=TRUE))
print(sum((entity_df$Internet.Use == 1) & (entity_df$Smartphone == 1), na.rm=TRUE))
print(sum((entity_df$Internet.Use == 1) & (entity_df$Smartphone == 0), na.rm=TRUE))
print(sum((entity_df$Internet.Use == 0) & (entity_df$Smartphone == 1), na.rm=TRUE))

entity_limited_df <- subset(entity_df, (Internet.Use == 1) | (Smartphone == 1))
print(nrow(entity_limited_df))
print(sapply(names(entity_limited_df), function(col) sum(is.na(entity_limited_df[, col]))))
#print(sum(is.na(entity_limited_df$Internet.Use)))

print(summary(entity_limited_df))
print(Info_On_Iternet_freq_entity_limited_df <- mycreate_tbl_df(entity_limited_df, "Info.On.Internet"))
print(Worry_About_Info_freq_entity_limited_df <- mycreate_tbl_df(entity_limited_df, "Worry.About.Info"))
print(Worry_About_Info_freq_entity_limited_df[1, ".freq"] * 1.0 / sum(Worry_About_Info_freq_entity_limited_df[, ".freq"]))
print(Anonymity_Possible_freq_entity_limited_df <- mycreate_tbl_df(entity_limited_df, "Anonymity.Possible"))
print(Anonymity_Possible_freq_entity_limited_df[1, ".freq"] * 1.0 / sum(Anonymity_Possible_freq_entity_limited_df[, ".freq"]))
print(Tried_Masking_Identity_freq_entity_limited_df <- mycreate_tbl_df(entity_limited_df, "Tried.Masking.Identity"))
print(Tried_Masking_Identity_freq_entity_limited_df[1, ".freq"] * 1.0 / sum(Tried_Masking_Identity_freq_entity_limited_df[, ".freq"]))
print(Privacy_Laws_Effective_freq_entity_limited_df <- mycreate_tbl_df(entity_limited_df, "Privacy.Laws.Effective"))
print(Privacy_Laws_Effective_freq_entity_limited_df[1, ".freq"] * 1.0 / sum(Privacy_Laws_Effective_freq_entity_limited_df[, ".freq"]))

# Problem 4.*
print(Age_Info.On.Internet_xtab_entity_df <- 
  mycreate_xtab(entity_limited_df, c("Age", "Info.On.Internet")))
#print(max(Age_Info.On.Internet_xtab_entity_df, na.rm=TRUE))
print(sort(table(entity_limited_df$Age, entity_limited_df$Info.On.Internet)))
print(Info.On.Internet_by_Smartphone_arr <- 
   sort(tapply(entity_df$Info.On.Internet, entity_df$Smartphone, mean, na.rm=TRUE)))
print(Tried.Masking.Identity_by_Smartphone_arr <- 
   sort(tapply(entity_df$Tried.Masking.Identity, entity_df$Smartphone, mean, na.rm=TRUE)))

# Other plots:
print(myplot_histogram(entity_limited_df, "Age"))
# print(myplot_box(df=entity_df, ycol_names="<col1_name>"))
# print(myplot_box(df=entity_df, ycol_names="<col1_name>", xcol_name="<col2_name>"))
# print(myplot_line(subset(entity_df, Symbol %in% c("KO", "PG")), 
#                   "Date.my", "StockPrice", facet_row_colnames="Symbol") + 
#     geom_vline(xintercept=as.numeric(as.Date("2003-03-01"))) +
#     geom_vline(xintercept=as.numeric(as.Date("1983-01-01")))        
#         )
print(myplot_scatter(entity_limited_df, "Age", "Info.On.Internet"))

script_df <- rbind(script_df, 
    data.frame(chunk_label="manage_missing_data", 
        chunk_step_major=max(script_df$chunk_step_major), 
        chunk_step_minor=script_df[which.max(script_df$chunk_step_major), 
                                   "chunk_step_minor"]+1))
print(script_df)
```

### Step ``r script_df[nrow(script_df), "chunk_step_major"]``.``r script_df[nrow(script_df), "chunk_step_minor"]``: manage missing data
```{r manage_missing_data_1, cache=TRUE}

script_df <- rbind(script_df, 
    data.frame(chunk_label="encode_data", 
        chunk_step_major=max(script_df$chunk_step_major), 
        chunk_step_minor=script_df[which.max(script_df$chunk_step_major), 
                                   "chunk_step_minor"]+1))
print(script_df)
```

### Step ``r script_df[nrow(script_df), "chunk_step_major"]``.``r script_df[nrow(script_df), "chunk_step_minor"]``: encode data
```{r encode_data_1, cache=TRUE}
# map_<col_name>_df <- myimport_data(
#     url="<map_url>", 
#     comment="map_<col_name>_df", print_diagn=TRUE)
# 
# entity_df <- mymap_codes(entity_df, "<from_col_name>", "<to_col_name>", 
#     map_<to_col_name>_df, map_join_col_name="<map_join_col_name>", 
#                           map_tgt_col_name="<to_col_name>")
    					
script_df <- rbind(script_df, 
                   data.frame(chunk_label="extract_features", 
                              chunk_step_major=max(script_df$chunk_step_major)+1, 
                              chunk_step_minor=0))
print(script_df)
```

## Step ``r script_df[nrow(script_df), "chunk_step_major"]``: extract features
```{r extract_features, cache=TRUE}

# script_df <- rbind(script_df, 
#                    data.frame(chunk_label="extract_features", 
#                               chunk_step_major=max(script_df$chunk_step_major)+1, 
#                               chunk_step_minor=0))
print(script_df)
```

Null Hypothesis ($\sf{H_{0}}$): mpg is not impacted by am_fctr.  
The variance by am_fctr appears to be independent. 
```{r q1, cache=TRUE}
# print(t.test(subset(cars_df, am_fctr == "automatic")$mpg, 
#              subset(cars_df, am_fctr == "manual")$mpg, 
#              var.equal=FALSE)$conf)
```
We reject the null hypothesis i.e. we have evidence to conclude that am_fctr impacts mpg (95% confidence). Manual transmission is better for miles per gallon versus automatic transmission.

## remove nearZeroVar features (not much variance)
#require(reshape)
#var_features_df <- melt(summaryBy(. ~ factor(0), data=entity_df[, features_lst], 
#                             FUN=var, keep.names=TRUE), 
#                             variable_name=c("feature"))
#names(var_features_df)[2] <- "var"
#print(var_features_df[order(var_features_df$var), ])
# summaryBy ignores factors whereas nearZeroVar inspects factors

# k_fold <- 5
# entity_df[order(entity_df$classe, 
#                   entity_df$user_name, 
#                   entity_df$my.rnorm),"my.cv_ix"] <- 
#     rep(1:k_fold, length.out=nrow(entity_df))
# summaryBy(X ~ my.cv_ix, data=entity_df, FUN=length)
# tapply(entity_df$X, list(entity_df$classe, entity_df$user_name, 
#                            entity_df$my.cv_ix), length)

#require(DAAG)
#entity_df$classe.proper <- as.numeric(entity_df$classe == "A")
#rnorm.glm <- glm(classe.proper ~ rnorm, family=binomial, data=entity_df)
#cv.binary(rnorm.glm, nfolds=k_fold, print.details=TRUE)
#result <- cv.lm(df=entity_df, form.lm=formula(classe ~ rnorm), 
#                    m=k_fold, seed=12345, printit=TRUE)

#plot(mdl_1$finalModel, uniform=TRUE, main="base")
#text(mdl_1$finalModel, use.n=TRUE, all=TRUE, cex=0.8)


```{r print_sessionInfo, echo=FALSE}
sessionInfo()
```